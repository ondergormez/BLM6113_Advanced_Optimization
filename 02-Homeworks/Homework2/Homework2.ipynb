{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: MNIST Dataset\n",
    "Ad Soyad: Önder Görmez  \n",
    "Öğrenci No: 21501035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "# Soru 1\n",
    "Bu problemde L2 düzenlileştirilmesi ne işe yaramaktadır? Buradaki, γ hiperparametresinin artışı ve azalışı teorik açıdan ne sağlamaktadır? Araştırarak raporlayınız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regularization modelin overfitting olmaması için coefficient tahminini 0 a yakınlaştırmaktır.\n",
    "* Böylelikle model overfitting nedeniyle düzgün çalışmadığı zaman, modelin karşmaşıklığını kontrol altında tutabiliriz.\n",
    "* Teknik olarak regularization, overfitting i modelin loss fonksiyonuna bir penaltı değeri ekleyerek engeller.\n",
    "\n",
    "$$\n",
    "\\begin {array}{|l|}\n",
    "\\hline\n",
    "Regularization = Loss Function + Penalty \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "${\\gamma}$ : Regularization hiper parametresi.\n",
    "* Regularizasyon un kuvvetini kontrol eder.\n",
    "Büyük ${\\gamma}$ daha fazla regularizasyon, modelde küçük coefficient anlamına gelir. Fakat öğrenmenin çok azalmasına (underfitting) neden olabilir.\n",
    "* ${\\gamma}$ sonsuza gittikçe underfitting gerçekleşir ve model mevcut datalar üzerinden öğrenme gerçekleştirememiş olur.\n",
    "* Küçük ${\\gamma}$ daha az regularizasyon anlamına gelmektedir. Lambda küçüldükçe modeli çok fazla öğrenme (overfitting) eğilimi artar.\n",
    "* ${\\gamma}$ sıfıra gittikça overfitting gerçekleşir.\n",
    "* ${\\gamma}$ sıfır olursa denklemler lineer regresyon denklemine dönüşür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "# Soru 2\n",
    "Yukarıda verilen maliyet fonksiyonun gradyen vektörü ve Hessian matrisini bulunuz. Elde ettiğiniz denklemlerle kodun ilgili kısmını karşılaştırarak doğrulama yapınız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin {array}{|l|}\n",
    "\\hline\n",
    "\\phi = Loss Function + Penalty \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 + 2*x + 4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Add' object has no attribute 'latex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m eq \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(eq)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43meq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatex\u001b[49m())\n\u001b[1;32m     23\u001b[0m diff \u001b[38;5;241m=\u001b[39m eq\u001b[38;5;241m.\u001b[39mdiff(x)\n\u001b[1;32m     24\u001b[0m diff\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Add' object has no attribute 'latex'"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "from sympy import init_printing\n",
    "init_printing()\n",
    "\n",
    "\n",
    "# define symbolic variable\n",
    "t = sp.symbols('t')\n",
    "\n",
    "# define symbolic equation\n",
    "eq = sp.exp(t / 2) * (sp.sin(t / 3) ** 2)\n",
    "eq\n",
    "\n",
    "z = sp.symbols('z')\n",
    "sigmoid = 1 / (1 + sp.exp(-z))\n",
    "sigmoid\n",
    "\n",
    "\n",
    "x = sp.symbols('x')\n",
    "eq = x ** 2 + 2 * x + 4\n",
    "print(eq)\n",
    "\n",
    "diff = eq.diff(x)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac{1}{1 + e^{- z}}\n",
      "x^{2} + 2 x + 4\n"
     ]
    }
   ],
   "source": [
    "# https://docs.sympy.org/latest/tutorials/intro-tutorial/printing.html\n",
    "\n",
    "from sympy.printing.latex import latex\n",
    "\n",
    "print(latex(sigmoid))\n",
    "print(latex(eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{e^{- z}}{\\left(1 + e^{- z}\\right)^{2}}$"
      ],
      "text/plain": [
       "exp(-z)/(1 + exp(-z))**2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fod: first order derivative\n",
    "sigmoid_fod = sigmoid.diff(z)\n",
    "sigmoid_fod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "# Soru 3\n",
    "target_ind_0=0 ve target_ind_1=1 iken öğrenme oranı sabit (lr=0.1) olacak şekilde aşağıdaki yöntemleri kodlayınız ve performanslarını aynı grafik üzerinde karşılaştırınız. Ayrıca, kaç saniye sürdüğü bilgisini de paylaşınız. Test doğruluğu yöntemlere göre nasıl değişmektedir, inceleyiniz. Gradyen azalım yönteminin de karşılaştırmada bulunması gerekmektedir. Not: Kodun plot ile sağladığı iki farklı grafiği de eklemeniz gerekmektedir. \n",
    "\n",
    "•\tModifiye Edilmiş Newton  \n",
    "•\tConjugate Gradient  \n",
    "•\tLevenberg-Marquardt  \n",
    "•\tBFGS  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "# Soru 4\n",
    "Armijo stepsize search ile değişken olarak belirlenen öğrenme oranı kullanarak Gradyen Azalım, Modifiye Edilmiş Newton ve Conjugate Gradyen yöntemlerini karşılaştırınız. Değişken olarak belirlenen öğrenme oranı sabit duruma göre nasıl bir katkı sağlamıştır?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3_12_2_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
