{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Week - 3 May 2024 Friday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu derste CNN ile Optimizasyonu karşılaştıracağız.\n",
    "\n",
    "# Convolutional Neural Networks (CNN)\n",
    "* Convolution 1 - (5 x 5) kernel, 24 x 24 filter, n1 channels (n1 convolutional layers or n1 filters)\n",
    "* Max Pooling 1 - (2 x 2), 12 x 12 filter, n1 channels\n",
    "* Convolution 2 - (5 x 5) kernel, 8 x 8  filter, n2 channels\n",
    "* Max Pooling 2 - (2 x 2), 4 x 4 filter, n2 channels\n",
    "* Fully Connected Layer - 4 x 4 x n2 neurons (ReLU burada kullanılır)\n",
    "* Output Layer - 10 neurons\n",
    "\n",
    "\n",
    "ReLU: Rectified Linear Unit\n",
    "> In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:  \n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1b43bb9d7f851aa7c8ce8e4dacbd943d18512528)  \n",
    "Source: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "Max Pooling (sub sampling): En önemli değerleri alarak konvolisyon katmanının boyutunu azaltıyoruz diyebilirz.\n",
    "\n",
    "\n",
    "Derin öğrenmede artık kernel ler initial rastgele bir ağırlık (değerler bütünü) ile başlıyor. Bunu iteratif şekilde değiştirerek en iyiye gitmeye çalışıyor diyebiliriz.\n",
    "* Pytorch veya tensorflow gibi kütüphanelerde convolution işlemi için kullanılan kütüphanenin isminin farklı olduğunu görürsünüz. 2D Cross Correlation terimini kullanırlar.\n",
    "* Convolution ı cross correlation fonksiyonu ile hesaplayabilmek için (i - m, j - n) şeklinde yazmak gerekir.\n",
    "    * filtreyi x ve y yönünde aynalayarak ilerlemek gerekir.\n",
    "\n",
    "\n",
    "filtreye ne koyduğuna bağlı olarak kenar, köşe bulan gibi farklı uygulamalar yapılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layers\n",
    "\n",
    "\n",
    "## Max Pooling Layer\n",
    "\n",
    "* Kayan pencereler şeklinde uygulanarak değerler elde edilir.\n",
    "* Maksimum değer o anki değer olarak kullanılır.\n",
    "\n",
    "\n",
    "## Sigmoid Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation for Convolutional Neural Networks\n",
    "* Çıktıyı alıp geriye doğru giderek ağırlıkları güncellemek için kullanılır.\n",
    "\n",
    "\n",
    "Bizim Loss fonksiyonumuzu minimize etmemez lazım. Backpropagation bunun için kullanılır.\n",
    "\n",
    "## Copilot ile Back Propagation\n",
    "\n",
    "Öncelikle, bir sinir ağı için backpropagation algoritmasını uygulamak için bir hata (loss) fonksiyonuna ihtiyacınız olacak. Bu, ağın çıktısının beklenen çıktıdan ne kadar farklı olduğunu ölçer. Ortak hata fonksiyonları arasında Mean Squared Error (MSE) ve Cross-Entropy bulunur.\n",
    "\n",
    "Python dilinde, bir örnek olarak Mean Squared Error (MSE) hata fonksiyonunu aşağıdaki gibi yazabiliriz:\n",
    "\n",
    "```python\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "```\n",
    "\n",
    "$$ L(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "Bu fonksiyon, gerçek değerler (y_true) ve ağın tahminleri (y_pred) arasındaki kare farkın ortalamasını hesaplar. Bu, regresyon problemları için genellikle kullanılır.\n",
    "\n",
    "Eğer bir sınıflandırma problemi üzerinde çalışıyorsanız, Cross-Entropy hata fonksiyonunu kullanabilirsiniz:\n",
    "\n",
    "```python\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "```\n",
    "\n",
    "Bu fonksiyon, gerçek değerler (y_true) ve ağın tahminleri (y_pred) arasındaki çapraz entropiyi hesaplar. Bu, sınıflandırma problemları için genellikle kullanılır.\n",
    "\n",
    "Her iki durumda da, backpropagation algoritması hata fonksiyonunun gradyanını (türevini) hesaplar ve bu bilgiyi kullanarak ağın ağırlıklarını günceller. Bu, ağın çıktısını beklenen çıktıya daha da yaklaştırır.\n",
    "\n",
    "$$ L(y, \\hat{y}) = -\\sum_{i} y_i \\log(\\hat{y}_i) $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
