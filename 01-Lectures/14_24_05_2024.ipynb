{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Week - 24 May 2024 Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ödev Açıklaması\n",
    "AdamW\n",
    "\n",
    "Weight Decay\n",
    "\n",
    "* Makaleye baktığınızda adım adım neler üzerine değinilmiş\n",
    "\n",
    "Adam ve RMSProp daki eksikliklerin neler olduğunu makalede açıklamış  \n",
    "Makalede kendinizle ilgili olan noktalara odaklanın.  \n",
    "Tüm detayları ispatları anlamanıza gerek yok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization (Kısıtlı Optimizasyon)\n",
    "Son 2 senede anlatılmayan bir konuya değiniyoruz.\n",
    "\n",
    "Inactive: Kısıtı koymanın hiçbir şeyi değiştirmediği durum. Buda minimum zaten kısıtın içerisinde bir yerde kalıyor demektir.  \n",
    "Active: Kısıtı koymanın minimum noktayı başka bir yere çektiği durum.\n",
    "\n",
    "\n",
    "\n",
    "# Son Not\n",
    "Veri bilimi için olasılık, optimizasyon, lineer cebir önemli konular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Sınavına Hazırlık\n",
    "TODO: Verilen ödevlerin çözümlerini tekrar et. Bunlar hakkında final sınavında soru sorulabilir.\n",
    "\n",
    "# Final Soruları (115 puan)\n",
    "\n",
    "## Soru 1 (25 puan)\n",
    "a) Rosen brock fonksiyonu aşağıda verilen  \n",
    "... Normal bir denklem...  \n",
    "\n",
    "fonksiyonunu Conjugate Gradient (CG) ile 2 iterasyonda çözünüz.  \n",
    "0.1 ve initial 0,0\n",
    "\n",
    "\n",
    "## Soru 2 (30 puan)\n",
    "Aşağıdaki soruları kısaca cevaplayınız.  \n",
    "a) Gradient descent yerine neden SGD kullanılır? (7 puan)  \n",
    "b) Momentum yöntemi düzlüklerde çukurlardan çıkmaya yarar mı neden? (8 puan)  \n",
    "c) Adagrad yöntemi nasıl çalışır? Sorun lu olduğu nokta/dezavantajı nedir? (7 puan)  \n",
    "d) Adam yönteminin nasıl çalıştığını açıklayınız? (8 puan)\n",
    "\n",
    "\n",
    "## Soru 3 (35 puan)\n",
    "Kısıtlı optimizasyon\n",
    "\n",
    "a) Langrange çarpanları yöntemiyle çözünüz. çözüm noktasında;  \n",
    " grandyent fx = lamda x gradient gx  \n",
    "in eşit çıktığını gösteriniz. (25 puan)\n",
    "\n",
    "b) bir şey todo:, log-bariyer ve augmentted yöntemlerinin herhangi biriyle çözülse nasıl olurdu?  \n",
    "Denklem yazınız ve izlenen yolları kısaca adım adım açıklayınız. (10 puan)\n",
    "\n",
    "\n",
    "## Soru 4 (25 puan)\n",
    "a) Kendi oluşturduğunuz 2 bilinmeyenli bir denkleme L1 ve L2 regularization yöntemleri ekleyiniz.  \n",
    "Bu durumda cost fonksiyonunun ne duruma geldiğini yazınız.  \n",
    "L1 ve L2 ne sağlar açıklayınız?  \n",
    "L1 neden sparse bir sonuç üretir?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
