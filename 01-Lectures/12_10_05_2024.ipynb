{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Week - 10 May 2024 Friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent: Multiple Training Sample\n",
    "* Veri setiniz çok küçükse batch size ı daha büyük seçmeniz gerekmektedir.\n",
    "* Tüm veriyi kullanmanız lazım.\n",
    "\n",
    "\n",
    "## Batch GD = Vanilla GD\n",
    "* Büyük veride yavaş\n",
    "* Memory e sığmayabilir\n",
    "* Sağa sola gitmeden düm düz ilerler\n",
    "\n",
    "## Stochastic Gradient Descent (SGD):\n",
    "* Random veri örnekleri ile gradient hesabı yapılır.\n",
    "* Avantajı hızlı hesaplama\n",
    "* Dezavantajı large noise to the gradient descend\n",
    "* Algoritmada veri ilk başta shuffle edilir.\n",
    "\n",
    "\n",
    "## Mini Batch Gradient Descent:\n",
    "* batch size 32 veya 16 gibi değerler olabilir. TODO: Başka hangi değerler olur?\n",
    "* En çok kullanılan algoritmadır. Batch size ile ilgili bir bilgi veriliyorsa, genelde bu algoritma için verilmiştir.\n",
    "\n",
    "![](https://statusneo.com/wp-content/uploads/2023/09/Credit-Analytics-Vidya.jpg)\n",
    "\n",
    "Image Source: https://statusneo.com/wp-content/uploads/2023/09/Credit-Analytics-Vidya.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant of Gradient Descent Algorithms\n",
    "* Momentum\n",
    "* Nesterov accelerated gradient\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* RMSprop\n",
    "* Adam\n",
    "* Adam extensions\n",
    "\n",
    "\n",
    "saddle: düz plato olan yerler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Momentum\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/0*TKxSMrG2xPLtcRVy.png)  \n",
    "Image Source: https://miro.medium.com/v2/resize:fit:1400/0*TKxSMrG2xPLtcRVy.png\n",
    "\n",
    "\n",
    "* Daha önceden hesaplanmış gradient yönünde bir yönelim varsa bu eklenerek yeni gradient hesabı yapılır.\n",
    "* Böylelikle fizikte yukarıdan serbest bırakılan bir kütle gibi önce hızlanarak bir moment elde edilir.\n",
    "* Sonrasında düzlüğe gelinse de durmayacağı ve biraz daha ilerleyeceği için küçük düzlükler varsa buraları kolay bir şekilde atlayabilir. Daha aşağıda bulunan düzlüklere ilerleyebilir.\n",
    "\n",
    "\n",
    "![](https://gbhat.com/assets/gifs/sgd_momentum_nesterov.gif)\n",
    "Image Source: https://gbhat.com/assets/gifs/sgd_momentum_nesterov.gif\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "\n",
    "Yukarıda grafiği mevcut\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad Update\n",
    "\n",
    "* Learning rate i adaptif bir şekilde değiştirir.\n",
    "* İlk başta 1 gibi bir değerle başlar ve sonrasında giderek küçülür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp (Leak AdaGrad)\n",
    "\n",
    "RMSProp: Root Mean Square Propagation\n",
    "\n",
    "* Local minimumlardan kaçma gibi bir özelliği yok.\n",
    "* Gradient i fazla olan yönün learning rate i düşük olur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam)\n",
    "* Combines Momentum and RMSProp\n",
    "* 0/0 belirsizliği oluşturmamak için bir düzeltme terimi eklenmiştir.\n",
    "* Standard: Adam is a good choise for default in practice\n",
    "* Fallback option: SGD with momentum"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
